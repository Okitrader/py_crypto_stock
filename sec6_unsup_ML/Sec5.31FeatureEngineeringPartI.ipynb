{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f5abd8-921a-4e3e-aa80-37ee6c204726",
   "metadata": {},
   "source": [
    "# Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a837c4a-ec38-49e7-8984-6161913d7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Management\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pandas_datareader.data import DataReader\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "#Statistics\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Data Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Supervised Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Reporting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767552d-8a2c-4c44-987d-8639324af379",
   "metadata": {},
   "source": [
    "# Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35172e61-ef5c-4463-811c-8d7a48c8a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data: 199504\n",
      "shape: (5, 9)\n",
      "┌────────────┬─────┬──────────────┬────────────┬───┬─────┬──────┬─────┬──────────┐\n",
      "│ Date       ┆ Id  ┆ suburb       ┆ postalCode ┆ … ┆ bed ┆ bath ┆ car ┆ propType │\n",
      "│ ---        ┆ --- ┆ ---          ┆ ---        ┆   ┆ --- ┆ ---  ┆ --- ┆ ---      │\n",
      "│ str        ┆ i64 ┆ str          ┆ i64        ┆   ┆ f64 ┆ i64  ┆ f64 ┆ str      │\n",
      "╞════════════╪═════╪══════════════╪════════════╪═══╪═════╪══════╪═════╪══════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 4.0 ┆ 2    ┆ 2.0 ┆ house    │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 4.0 ┆ 3    ┆ 4.0 ┆ house    │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 3.0 ┆ 3    ┆ 2.0 ┆ house    │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 3.0 ┆ 1    ┆ 2.0 ┆ house    │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 5.0 ┆ 4    ┆ 4.0 ┆ house    │\n",
      "└────────────┴─────┴──────────────┴────────────┴───┴─────┴──────┴─────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Set the file path\n",
    "file_path = \"/Users/okitrader/OneDrive/py_crypto_stock/SydneyHousePrices.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pl.read_csv(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(f'Length of Data: {len(df)}')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bccfed-cdc6-4473-a1cc-3597649efddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'propType']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b229bd14-d5a8-4e51-b5df-43562e482a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "OrderedDict({'Date': String, 'Id': Int64, 'suburb': String, 'postalCode': Int64, 'sellPrice': Int64, 'bed': Float64, 'bath': Int64, 'car': Float64, 'propType': String})\n",
      "\n",
      "Number of rows: 199504\n",
      "Number of columns: 9\n",
      "\n",
      "Null counts for each column:\n",
      "shape: (1, 9)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ Date_null ┆ Id_null_c ┆ suburb_nu ┆ postalCod ┆ … ┆ bed_null_ ┆ bath_null ┆ car_null_ ┆ propType │\n",
      "│ _count    ┆ ount      ┆ ll_count  ┆ e_null_co ┆   ┆ count     ┆ _count    ┆ count     ┆ _null_co │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ unt       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ unt      │\n",
      "│ u32       ┆ u32       ┆ u32       ┆ ---       ┆   ┆ u32       ┆ u32       ┆ u32       ┆ ---      │\n",
      "│           ┆           ┆           ┆ u32       ┆   ┆           ┆           ┆           ┆ u32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 154       ┆ 0         ┆ 18151     ┆ 0        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame schema\n",
    "print(\"Schema:\")\n",
    "print(df.schema)\n",
    "\n",
    "# Display number of rows and columns\n",
    "print(\"\\nNumber of rows:\", df.height)\n",
    "print(\"Number of columns:\", df.width)\n",
    "\n",
    "# Display null counts for each column\n",
    "print(\"\\nNull counts for each column:\")\n",
    "null_counts = df.select([pl.col(col).is_null().sum().alias(f\"{col}_null_count\") for col in df.columns])\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc7df1-598d-4dfb-babc-10f74a3d4219",
   "metadata": {},
   "source": [
    "# Feature Engineering - Common Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bf2bb-31d9-47be-a0a0-19f2f131b1ad",
   "metadata": {},
   "source": [
    "## Handle Non-Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95ee72a-0333-477a-85e5-07af74893c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Suburbs:  685\n",
      "Preform label encoding\n"
     ]
    }
   ],
   "source": [
    "# Count unique items for 'suburb'\n",
    "suburb_text_unique = df['suburb'].unique()\n",
    "suburb_text_unique_list = suburb_text_unique.to_list() # prints the full list for viewing\n",
    "print('Unique Suburbs: ', len(suburb_text_unique))\n",
    "print('Preform label encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06dde192-8fbe-4890-b4d5-cf4c247b659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Prop Types:  8\n",
      "Preform OneHotEncoding encoding\n"
     ]
    }
   ],
   "source": [
    "# Count unique items for propType\n",
    "prop_type_text_unique = df['propType'].unique()\n",
    "print('Unique Prop Types: ', len(prop_type_text_unique))\n",
    "print('Preform OneHotEncoding encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1603cb80-5331-48c6-bdd7-3301b5f243c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 10)\n",
      "┌────────────┬─────┬──────────────┬────────────┬───┬──────┬─────┬──────────┬─────────────────┐\n",
      "│ Date       ┆ Id  ┆ suburb       ┆ postalCode ┆ … ┆ bath ┆ car ┆ propType ┆ suburbs_encoded │\n",
      "│ ---        ┆ --- ┆ ---          ┆ ---        ┆   ┆ ---  ┆ --- ┆ ---      ┆ ---             │\n",
      "│ str        ┆ i64 ┆ str          ┆ i64        ┆   ┆ i64  ┆ f64 ┆ str      ┆ u32             │\n",
      "╞════════════╪═════╪══════════════╪════════════╪═══╪══════╪═════╪══════════╪═════════════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 2    ┆ 2.0 ┆ house    ┆ 22              │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 3    ┆ 4.0 ┆ house    ┆ 22              │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 3    ┆ 2.0 ┆ house    ┆ 654             │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 1    ┆ 2.0 ┆ house    ┆ 22              │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 4    ┆ 4.0 ┆ house    ┆ 654             │\n",
      "└────────────┴─────┴──────────────┴────────────┴───┴──────┴─────┴──────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for 'suburb'\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Perform label encoding on the 'suburb' column to convert categorical text data into numerical values\n",
    "# The LabelEncoder's fit_transform method fits the encoder and returns the transformed values as a NumPy array\n",
    "# This step is necessary for machine learning models which require numerical input data\n",
    "encoded_suburbs = labelencoder.fit_transform(df['suburb'].to_numpy())\n",
    "\n",
    "\n",
    "# Add the encoded column to the DataFrame\n",
    "df = df.with_columns(pl.Series(encoded_suburbs, dtype=pl.UInt32).alias(\"suburbs_encoded\"))\n",
    "\n",
    "# Display the first 5 rows after encoding\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f667e7c-ac7a-4ecf-8791-d7faea227ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 17)\n",
      "┌────────────┬─────┬────────┬────────────┬───┬──────────┬────────────┬────────────────┬────────────┐\n",
      "│ Date       ┆ Id  ┆ suburb ┆ postalCode ┆ … ┆ pt_other ┆ pt_terrace ┆ pt_warehouse   ┆ pt_acreage │\n",
      "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---      ┆ ---        ┆ ---            ┆ ---        │\n",
      "│ str        ┆ i64 ┆ str    ┆ i64        ┆   ┆ i32      ┆ i32        ┆ i32            ┆ i32        │\n",
      "╞════════════╪═════╪════════╪════════════╪═══╪══════════╪════════════╪════════════════╪════════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "└────────────┴─────┴────────┴────────────┴───┴──────────┴────────────┴────────────────┴────────────┘\n",
      "\n",
      "Columns after one-hot encoding:\n",
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'suburbs_encoded', 'pt_house', 'pt_townhouse', 'pt_duplex/semi-detached', 'pt_villa', 'pt_other', 'pt_terrace', 'pt_warehouse', 'pt_acreage']\n",
      "\n",
      "Unique values in 'pt_house' column:\n",
      "shape: (2,)\n",
      "Series: 'pt_house' [i32]\n",
      "[\n",
      "\t0\n",
      "\t1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for 'propType' using Polars\n",
    "oneshot_encoded = df.with_columns([\n",
    "    pl.when(pl.col('propType') == pt).then(1).otherwise(0).alias(f'pt_{pt}')\n",
    "    for pt in df['propType'].cast(pl.Categorical).unique()\n",
    "])\n",
    "\n",
    "# Drop the original 'propType' column\n",
    "oneshot_encoded = oneshot_encoded.drop('propType')\n",
    "\n",
    "# Display the first 5 rows after one-hot encoding\n",
    "print(oneshot_encoded.head())\n",
    "\n",
    "# Display the list of columns to verify one-hot encoding\n",
    "print(\"\\nColumns after one-hot encoding:\")\n",
    "print(oneshot_encoded.columns)\n",
    "\n",
    "# Check the unique values in one of the new columns\n",
    "pt_columns = [col for col in oneshot_encoded.columns if col.startswith('pt_')]\n",
    "if pt_columns:\n",
    "    first_pt_column = pt_columns[0]\n",
    "    print(f\"\\nUnique values in '{first_pt_column}' column:\")\n",
    "    print(oneshot_encoded[first_pt_column].unique())\n",
    "else:\n",
    "    print(\"\\nNo 'pt_' columns found. One-hot encoding may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0ab322-9b22-4c6d-b164-59289c6f7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(oneshot_encoded, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce5afea-2eb8-4fdc-bbeb-a3069b1ee25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 26)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Date</th><th>Id</th><th>suburb</th><th>postalCode</th><th>sellPrice</th><th>bed</th><th>bath</th><th>car</th><th>propType</th><th>suburbs_encoded</th><th>Date_right</th><th>suburb_right</th><th>postalCode_right</th><th>sellPrice_right</th><th>bed_right</th><th>bath_right</th><th>car_right</th><th>suburbs_encoded_right</th><th>pt_house</th><th>pt_townhouse</th><th>pt_duplex/semi-detached</th><th>pt_villa</th><th>pt_other</th><th>pt_terrace</th><th>pt_warehouse</th><th>pt_acreage</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;2019-06-19&quot;</td><td>1</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1210000</td><td>4.0</td><td>2</td><td>2.0</td><td>&quot;house&quot;</td><td>22</td><td>&quot;2019-06-19&quot;</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1210000</td><td>4.0</td><td>2</td><td>2.0</td><td>22</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-06-13&quot;</td><td>2</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>2250000</td><td>4.0</td><td>3</td><td>4.0</td><td>&quot;house&quot;</td><td>22</td><td>&quot;2019-06-13&quot;</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>2250000</td><td>4.0</td><td>3</td><td>4.0</td><td>22</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-06-07&quot;</td><td>3</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>2920000</td><td>3.0</td><td>3</td><td>2.0</td><td>&quot;house&quot;</td><td>654</td><td>&quot;2019-06-07&quot;</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>2920000</td><td>3.0</td><td>3</td><td>2.0</td><td>654</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-05-28&quot;</td><td>4</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1530000</td><td>3.0</td><td>1</td><td>2.0</td><td>&quot;house&quot;</td><td>22</td><td>&quot;2019-05-28&quot;</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1530000</td><td>3.0</td><td>1</td><td>2.0</td><td>22</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-05-22&quot;</td><td>5</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>8000000</td><td>5.0</td><td>4</td><td>4.0</td><td>&quot;house&quot;</td><td>654</td><td>&quot;2019-05-22&quot;</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>8000000</td><td>5.0</td><td>4</td><td>4.0</td><td>654</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 26)\n",
       "┌────────────┬─────┬────────┬────────────┬───┬──────────┬────────────┬────────────────┬────────────┐\n",
       "│ Date       ┆ Id  ┆ suburb ┆ postalCode ┆ … ┆ pt_other ┆ pt_terrace ┆ pt_warehouse   ┆ pt_acreage │\n",
       "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---      ┆ ---        ┆ ---            ┆ ---        │\n",
       "│ str        ┆ i64 ┆ str    ┆ i64        ┆   ┆ i32      ┆ i32        ┆ i32            ┆ i32        │\n",
       "╞════════════╪═════╪════════╪════════════╪═══╪══════════╪════════════╪════════════════╪════════════╡\n",
       "│ 2019-06-19 ┆ 1   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-06-13 ┆ 2   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-06-07 ┆ 3   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-05-28 ┆ 4   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-05-22 ┆ 5   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "└────────────┴─────┴────────┴────────────┴───┴──────────┴────────────┴────────────────┴────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029d855-00a0-4fd0-b4f8-a3e3c7537245",
   "metadata": {},
   "source": [
    "## Set Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a77afcf3-4d10-4480-b582-52bdcbedcf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 27)\n",
      "┌────────────┬─────┬────────┬────────────┬───┬────────────┬──────────────┬────────────┬─────────┐\n",
      "│ Date       ┆ Id  ┆ suburb ┆ postalCode ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET  │\n",
      "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---     │\n",
      "│ str        ┆ i64 ┆ str    ┆ i64        ┆   ┆ i32        ┆ i32          ┆ i32        ┆ i64     │\n",
      "╞════════════╪═════╪════════╪════════════╪═══╪════════════╪══════════════╪════════════╪═════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1210000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2250000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale  ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2920000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1530000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale  ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 8000000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "└────────────┴─────┴────────┴────────────┴───┴────────────┴──────────────┴────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'TARGET' that's a copy of 'sellPrice'\n",
    "df = df.with_columns(pl.col('sellPrice').alias('TARGET'))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af4a128-3b68-4590-88ef-8e314c6857bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'propType', 'suburbs_encoded', 'Date_right', 'suburb_right', 'postalCode_right', 'sellPrice_right', 'bed_right', 'bath_right', 'car_right', 'suburbs_encoded_right', 'pt_house', 'pt_townhouse', 'pt_duplex/semi-detached', 'pt_villa', 'pt_other', 'pt_terrace', 'pt_warehouse', 'pt_acreage', 'TARGET']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3cd9b-0b4e-4bb1-a18f-0fb58c8ccd1a",
   "metadata": {},
   "source": [
    "## Remove Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222a52ab-636d-46df-b4b0-0791b13627f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 22)\n",
      "┌────────────┬─────┬──────┬─────┬───┬────────────┬──────────────┬────────────┬─────────┐\n",
      "│ postalCode ┆ bed ┆ bath ┆ car ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET  │\n",
      "│ ---        ┆ --- ┆ ---  ┆ --- ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---     │\n",
      "│ i64        ┆ f64 ┆ i64  ┆ f64 ┆   ┆ i32        ┆ i32          ┆ i32        ┆ i64     │\n",
      "╞════════════╪═════╪══════╪═════╪═══╪════════════╪══════════════╪════════════╪═════════╡\n",
      "│ 2107       ┆ 4.0 ┆ 2    ┆ 2.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1210000 │\n",
      "│ 2107       ┆ 4.0 ┆ 3    ┆ 4.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2250000 │\n",
      "│ 2107       ┆ 3.0 ┆ 3    ┆ 2.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2920000 │\n",
      "│ 2107       ┆ 3.0 ┆ 1    ┆ 2.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1530000 │\n",
      "│ 2107       ┆ 5.0 ┆ 4    ┆ 4.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 8000000 │\n",
      "└────────────┴─────┴──────┴─────┴───┴────────────┴──────────────┴────────────┴─────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/6x3yk1r13xvcwchd72ftlnsc0000gn/T/ipykernel_38780/609900798.py:6: DeprecationWarning: named `columns` param is deprecated; use positional `*args` instead.\n",
      "  df_drop = df_drop.drop(columns=columns_to_remove)\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the DataFrame (Polars handles this internally)\n",
    "df_drop = df.clone()\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_remove = [\"Date\", \"Id\", \"suburb\", \"propType\", \"sellPrice\"]\n",
    "df_drop = df_drop.drop(columns=columns_to_remove)\n",
    "\n",
    "# Display the first 5 rows after dropping the columns\n",
    "print(df_drop.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2514117-8297-4b8e-8d3e-3b76120bdb63",
   "metadata": {},
   "source": [
    "## Check null or inf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01607878-c1ca-4281-8262-00fa5cc52f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/6x3yk1r13xvcwchd72ftlnsc0000gn/T/ipykernel_38780/469196574.py:4: DeprecationWarning: The `axis` parameter for `DataFrame.sum` is deprecated. Use `DataFrame.sum_horizontal()` to perform horizontal aggregation.\n",
      "  is_null = df.with_columns(pl.all().is_null().any()).sum(axis=1).sum() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for Null values across the DataFrame\n",
    "is_null = df.with_columns(pl.all().is_null().any()).sum(axis=1).sum() > 0\n",
    "print(\"Is Null: \", is_null)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4ff03a7-82df-44b2-946d-c86a81afa129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null: True\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assuming 'df' is your existing Polars DataFrame\n",
    "# Check for Null values\n",
    "contains_null = df.select(pl.any_horizontal(pl.all().is_null().any())).item()\n",
    "\n",
    "# Print the result for Null values\n",
    "print(\"Is Null:\", contains_null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a3c4255-1228-43ea-9d70-4de4bda0dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Inf: False\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Check for Infinite values, only applying it to numeric columns\n",
    "#this will error if you try to run on string columns\n",
    "contains_inf = df.select(\n",
    "    pl.any_horizontal(\n",
    "        pl.col(pl.Float64).is_infinite().any()\n",
    "    )\n",
    ").item()\n",
    "\n",
    "# Print the result for Infinite values\n",
    "print(\"Is Inf:\", contains_inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67b90623-504a-4f31-8077-cde8a663befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 27)\n",
      "┌────────────┬─────┬─────────────┬────────────┬───┬────────────┬─────────────┬────────────┬────────┐\n",
      "│ Date       ┆ Id  ┆ suburb      ┆ postalCode ┆ … ┆ pt_terrace ┆ pt_warehous ┆ pt_acreage ┆ TARGET │\n",
      "│ ---        ┆ --- ┆ ---         ┆ ---        ┆   ┆ ---        ┆ e           ┆ ---        ┆ ---    │\n",
      "│ str        ┆ f64 ┆ str         ┆ f64        ┆   ┆ f64        ┆ ---         ┆ f64        ┆ f64    │\n",
      "│            ┆     ┆             ┆            ┆   ┆            ┆ f64         ┆            ┆        │\n",
      "╞════════════╪═════╪═════════════╪════════════╪═══╪════════════╪═════════════╪════════════╪════════╡\n",
      "│ 2019-06-19 ┆ 1.0 ┆ Avalon      ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 1.21e6 │\n",
      "│            ┆     ┆ Beach       ┆            ┆   ┆            ┆             ┆            ┆        │\n",
      "│ 2019-06-13 ┆ 2.0 ┆ Avalon      ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 2.25e6 │\n",
      "│            ┆     ┆ Beach       ┆            ┆   ┆            ┆             ┆            ┆        │\n",
      "│ 2019-06-07 ┆ 3.0 ┆ Whale Beach ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 2.92e6 │\n",
      "│ 2019-05-28 ┆ 4.0 ┆ Avalon      ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 1.53e6 │\n",
      "│            ┆     ┆ Beach       ┆            ┆   ┆            ┆             ┆            ┆        │\n",
      "│ 2019-05-22 ┆ 5.0 ┆ Whale Beach ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 8e6    │\n",
      "└────────────┴─────┴─────────────┴────────────┴───┴────────────┴─────────────┴────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Calculate means for numeric columns\n",
    "numeric_cols = df.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
    "column_means = df.select([pl.col(col).mean() for col in numeric_cols])\n",
    "\n",
    "# Fill NA values with means for numeric columns\n",
    "df_filled = df.with_columns([\n",
    "    pl.col(col).fill_null(column_means.get_column(col)[0])\n",
    "    for col in numeric_cols\n",
    "])\n",
    "\n",
    "print(df_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d4eac3b-e8cc-4839-9d6e-72e267c563af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null: False\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assuming 'df' is your existing Polars DataFrame\n",
    "# Check for Null values\n",
    "contains_null = df_filled.select(pl.any_horizontal(pl.all().is_null().any())).item()\n",
    "\n",
    "# Print the result for Null values\n",
    "print(\"Is Null:\", contains_null)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981676a-b9a5-4f3e-b042-08816ce007ad",
   "metadata": {},
   "source": [
    "## Feature Scaling - Min Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a4c19f9-d17a-4eba-9623-78512f560acf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2019-06-19'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert the numeric DataFrame to a numpy array, apply scaling, and convert back to DataFrame\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# This step involves only numeric columns\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m df_scaled \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame(scaled_data, columns\u001b[38;5;241m=\u001b[39mnumeric_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Adding back the excluded column if needed\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# If you need to retain the non-numeric data alongside the scaled numeric data, you can join it back\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2019-06-19'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filtering out non-numeric columns to avoid conversion errors\n",
    "# In this example, 'date' column is assumed to be the non-numeric field that needs to be excluded\n",
    "numeric_df = df.select(pl.col(\"*\").exclude([\"date\"]))  # Replace \"date\" with the actual name of your non-numeric column\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Convert the numeric DataFrame to a numpy array, apply scaling, and convert back to DataFrame\n",
    "# This step involves only numeric columns\n",
    "scaled_data = scaler.fit_transform(numeric_df.to_numpy())\n",
    "df_scaled = pl.DataFrame(scaled_data, columns=numeric_df.columns)\n",
    "\n",
    "# Adding back the excluded column if needed\n",
    "# If you need to retain the non-numeric data alongside the scaled numeric data, you can join it back\n",
    "df_scaled = df_scaled.with_column(df.select(\"date\"))  # Adjust this if you have more than one non-numeric column\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(df_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea8875-5072-4fca-b493-efce0c8690cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
