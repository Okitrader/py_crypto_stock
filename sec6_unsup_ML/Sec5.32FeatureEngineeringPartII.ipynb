{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f5abd8-921a-4e3e-aa80-37ee6c204726",
   "metadata": {},
   "source": [
    "# Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a837c4a-ec38-49e7-8984-6161913d7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Management\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pandas_datareader.data import DataReader\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "#Statistics\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Data Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Supervised Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Reporting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767552d-8a2c-4c44-987d-8639324af379",
   "metadata": {},
   "source": [
    "# Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35172e61-ef5c-4463-811c-8d7a48c8a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data: 199504\n",
      "shape: (5, 9)\n",
      "┌────────────┬─────┬──────────────┬────────────┬───┬─────┬──────┬─────┬──────────┐\n",
      "│ Date       ┆ Id  ┆ suburb       ┆ postalCode ┆ … ┆ bed ┆ bath ┆ car ┆ propType │\n",
      "│ ---        ┆ --- ┆ ---          ┆ ---        ┆   ┆ --- ┆ ---  ┆ --- ┆ ---      │\n",
      "│ str        ┆ i64 ┆ str          ┆ i64        ┆   ┆ f64 ┆ i64  ┆ f64 ┆ str      │\n",
      "╞════════════╪═════╪══════════════╪════════════╪═══╪═════╪══════╪═════╪══════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 4.0 ┆ 2    ┆ 2.0 ┆ house    │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 4.0 ┆ 3    ┆ 4.0 ┆ house    │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 3.0 ┆ 3    ┆ 2.0 ┆ house    │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 3.0 ┆ 1    ┆ 2.0 ┆ house    │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 5.0 ┆ 4    ┆ 4.0 ┆ house    │\n",
      "└────────────┴─────┴──────────────┴────────────┴───┴─────┴──────┴─────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Set the file path\n",
    "file_path = \"/Users/okitrader/OneDrive/py_crypto_stock/SydneyHousePrices.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pl.read_csv(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(f'Length of Data: {len(df)}')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bccfed-cdc6-4473-a1cc-3597649efddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'propType']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b229bd14-d5a8-4e51-b5df-43562e482a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "OrderedDict({'Date': String, 'Id': Int64, 'suburb': String, 'postalCode': Int64, 'sellPrice': Int64, 'bed': Float64, 'bath': Int64, 'car': Float64, 'propType': String})\n",
      "\n",
      "Number of rows: 199504\n",
      "Number of columns: 9\n",
      "\n",
      "Null counts for each column:\n",
      "shape: (1, 9)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ Date_null ┆ Id_null_c ┆ suburb_nu ┆ postalCod ┆ … ┆ bed_null_ ┆ bath_null ┆ car_null_ ┆ propType │\n",
      "│ _count    ┆ ount      ┆ ll_count  ┆ e_null_co ┆   ┆ count     ┆ _count    ┆ count     ┆ _null_co │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ unt       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ unt      │\n",
      "│ u32       ┆ u32       ┆ u32       ┆ ---       ┆   ┆ u32       ┆ u32       ┆ u32       ┆ ---      │\n",
      "│           ┆           ┆           ┆ u32       ┆   ┆           ┆           ┆           ┆ u32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 154       ┆ 0         ┆ 18151     ┆ 0        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame schema\n",
    "print(\"Schema:\")\n",
    "print(df.schema)\n",
    "\n",
    "# Display number of rows and columns\n",
    "print(\"\\nNumber of rows:\", df.height)\n",
    "print(\"Number of columns:\", df.width)\n",
    "\n",
    "# Display null counts for each column\n",
    "print(\"\\nNull counts for each column:\")\n",
    "null_counts = df.select([pl.col(col).is_null().sum().alias(f\"{col}_null_count\") for col in df.columns])\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc7df1-598d-4dfb-babc-10f74a3d4219",
   "metadata": {},
   "source": [
    "# Feature Engineering - Common Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bf2bb-31d9-47be-a0a0-19f2f131b1ad",
   "metadata": {},
   "source": [
    "## Handle Non-Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95ee72a-0333-477a-85e5-07af74893c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Suburbs:  685\n",
      "Preform label encoding\n"
     ]
    }
   ],
   "source": [
    "# Count unique items for 'suburb'\n",
    "suburb_text_unique = df['suburb'].unique()\n",
    "suburb_text_unique_list = suburb_text_unique.to_list() # prints the full list for viewing\n",
    "print('Unique Suburbs: ', len(suburb_text_unique))\n",
    "print('Preform label encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06dde192-8fbe-4890-b4d5-cf4c247b659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Prop Types:  8\n",
      "Preform OneHotEncoding encoding\n"
     ]
    }
   ],
   "source": [
    "# Count unique items for propType\n",
    "prop_type_text_unique = df['propType'].unique()\n",
    "print('Unique Prop Types: ', len(prop_type_text_unique))\n",
    "print('Preform OneHotEncoding encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1603cb80-5331-48c6-bdd7-3301b5f243c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 10)\n",
      "┌────────────┬─────┬──────────────┬────────────┬───┬──────┬─────┬──────────┬─────────────────┐\n",
      "│ Date       ┆ Id  ┆ suburb       ┆ postalCode ┆ … ┆ bath ┆ car ┆ propType ┆ suburbs_encoded │\n",
      "│ ---        ┆ --- ┆ ---          ┆ ---        ┆   ┆ ---  ┆ --- ┆ ---      ┆ ---             │\n",
      "│ str        ┆ i64 ┆ str          ┆ i64        ┆   ┆ i64  ┆ f64 ┆ str      ┆ u32             │\n",
      "╞════════════╪═════╪══════════════╪════════════╪═══╪══════╪═════╪══════════╪═════════════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 2    ┆ 2.0 ┆ house    ┆ 22              │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 3    ┆ 4.0 ┆ house    ┆ 22              │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 3    ┆ 2.0 ┆ house    ┆ 654             │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon Beach ┆ 2107       ┆ … ┆ 1    ┆ 2.0 ┆ house    ┆ 22              │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale Beach  ┆ 2107       ┆ … ┆ 4    ┆ 4.0 ┆ house    ┆ 654             │\n",
      "└────────────┴─────┴──────────────┴────────────┴───┴──────┴─────┴──────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for 'suburb'\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Perform label encoding on the 'suburb' column to convert categorical text data into numerical values\n",
    "# The LabelEncoder's fit_transform method fits the encoder and returns the transformed values as a NumPy array\n",
    "# This step is necessary for machine learning models which require numerical input data\n",
    "encoded_suburbs = labelencoder.fit_transform(df['suburb'].to_numpy())\n",
    "\n",
    "\n",
    "# Add the encoded column to the DataFrame\n",
    "df = df.with_columns(pl.Series(encoded_suburbs, dtype=pl.UInt32).alias(\"suburbs_encoded\"))\n",
    "\n",
    "# Display the first 5 rows after encoding\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f667e7c-ac7a-4ecf-8791-d7faea227ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 17)\n",
      "┌────────────┬─────┬────────┬────────────┬───┬──────────┬────────────┬────────────────┬────────────┐\n",
      "│ Date       ┆ Id  ┆ suburb ┆ postalCode ┆ … ┆ pt_other ┆ pt_terrace ┆ pt_warehouse   ┆ pt_acreage │\n",
      "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---      ┆ ---        ┆ ---            ┆ ---        │\n",
      "│ str        ┆ i64 ┆ str    ┆ i64        ┆   ┆ i32      ┆ i32        ┆ i32            ┆ i32        │\n",
      "╞════════════╪═════╪════════╪════════════╪═══╪══════════╪════════════╪════════════════╪════════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
      "└────────────┴─────┴────────┴────────────┴───┴──────────┴────────────┴────────────────┴────────────┘\n",
      "\n",
      "Columns after one-hot encoding:\n",
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'suburbs_encoded', 'pt_house', 'pt_townhouse', 'pt_duplex/semi-detached', 'pt_villa', 'pt_other', 'pt_terrace', 'pt_warehouse', 'pt_acreage']\n",
      "\n",
      "Unique values in 'pt_house' column:\n",
      "shape: (2,)\n",
      "Series: 'pt_house' [i32]\n",
      "[\n",
      "\t0\n",
      "\t1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for 'propType' using Polars\n",
    "oneshot_encoded = df.with_columns([\n",
    "    pl.when(pl.col('propType') == pt).then(1).otherwise(0).alias(f'pt_{pt}')\n",
    "    for pt in df['propType'].cast(pl.Categorical).unique()\n",
    "])\n",
    "\n",
    "# Drop the original 'propType' column\n",
    "oneshot_encoded = oneshot_encoded.drop('propType')\n",
    "\n",
    "# Display the first 5 rows after one-hot encoding\n",
    "print(oneshot_encoded.head())\n",
    "\n",
    "# Display the list of columns to verify one-hot encoding\n",
    "print(\"\\nColumns after one-hot encoding:\")\n",
    "print(oneshot_encoded.columns)\n",
    "\n",
    "# Check the unique values in one of the new columns\n",
    "pt_columns = [col for col in oneshot_encoded.columns if col.startswith('pt_')]\n",
    "if pt_columns:\n",
    "    first_pt_column = pt_columns[0]\n",
    "    print(f\"\\nUnique values in '{first_pt_column}' column:\")\n",
    "    print(oneshot_encoded[first_pt_column].unique())\n",
    "else:\n",
    "    print(\"\\nNo 'pt_' columns found. One-hot encoding may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0ab322-9b22-4c6d-b164-59289c6f7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(oneshot_encoded, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce5afea-2eb8-4fdc-bbeb-a3069b1ee25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 26)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Date</th><th>Id</th><th>suburb</th><th>postalCode</th><th>sellPrice</th><th>bed</th><th>bath</th><th>car</th><th>propType</th><th>suburbs_encoded</th><th>Date_right</th><th>suburb_right</th><th>postalCode_right</th><th>sellPrice_right</th><th>bed_right</th><th>bath_right</th><th>car_right</th><th>suburbs_encoded_right</th><th>pt_house</th><th>pt_townhouse</th><th>pt_duplex/semi-detached</th><th>pt_villa</th><th>pt_other</th><th>pt_terrace</th><th>pt_warehouse</th><th>pt_acreage</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;2019-06-19&quot;</td><td>1</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1210000</td><td>4.0</td><td>2</td><td>2.0</td><td>&quot;house&quot;</td><td>22</td><td>&quot;2019-06-19&quot;</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1210000</td><td>4.0</td><td>2</td><td>2.0</td><td>22</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-06-13&quot;</td><td>2</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>2250000</td><td>4.0</td><td>3</td><td>4.0</td><td>&quot;house&quot;</td><td>22</td><td>&quot;2019-06-13&quot;</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>2250000</td><td>4.0</td><td>3</td><td>4.0</td><td>22</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-06-07&quot;</td><td>3</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>2920000</td><td>3.0</td><td>3</td><td>2.0</td><td>&quot;house&quot;</td><td>654</td><td>&quot;2019-06-07&quot;</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>2920000</td><td>3.0</td><td>3</td><td>2.0</td><td>654</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-05-28&quot;</td><td>4</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1530000</td><td>3.0</td><td>1</td><td>2.0</td><td>&quot;house&quot;</td><td>22</td><td>&quot;2019-05-28&quot;</td><td>&quot;Avalon Beach&quot;</td><td>2107</td><td>1530000</td><td>3.0</td><td>1</td><td>2.0</td><td>22</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;2019-05-22&quot;</td><td>5</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>8000000</td><td>5.0</td><td>4</td><td>4.0</td><td>&quot;house&quot;</td><td>654</td><td>&quot;2019-05-22&quot;</td><td>&quot;Whale Beach&quot;</td><td>2107</td><td>8000000</td><td>5.0</td><td>4</td><td>4.0</td><td>654</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 26)\n",
       "┌────────────┬─────┬────────┬────────────┬───┬──────────┬────────────┬────────────────┬────────────┐\n",
       "│ Date       ┆ Id  ┆ suburb ┆ postalCode ┆ … ┆ pt_other ┆ pt_terrace ┆ pt_warehouse   ┆ pt_acreage │\n",
       "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---      ┆ ---        ┆ ---            ┆ ---        │\n",
       "│ str        ┆ i64 ┆ str    ┆ i64        ┆   ┆ i32      ┆ i32        ┆ i32            ┆ i32        │\n",
       "╞════════════╪═════╪════════╪════════════╪═══╪══════════╪════════════╪════════════════╪════════════╡\n",
       "│ 2019-06-19 ┆ 1   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-06-13 ┆ 2   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-06-07 ┆ 3   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-05-28 ┆ 4   ┆ Avalon ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "│ 2019-05-22 ┆ 5   ┆ Whale  ┆ 2107       ┆ … ┆ 0        ┆ 0          ┆ 0              ┆ 0          │\n",
       "│            ┆     ┆ Beach  ┆            ┆   ┆          ┆            ┆                ┆            │\n",
       "└────────────┴─────┴────────┴────────────┴───┴──────────┴────────────┴────────────────┴────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029d855-00a0-4fd0-b4f8-a3e3c7537245",
   "metadata": {},
   "source": [
    "## Set Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77afcf3-4d10-4480-b582-52bdcbedcf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 27)\n",
      "┌────────────┬─────┬────────┬────────────┬───┬────────────┬──────────────┬────────────┬─────────┐\n",
      "│ Date       ┆ Id  ┆ suburb ┆ postalCode ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET  │\n",
      "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---     │\n",
      "│ str        ┆ i64 ┆ str    ┆ i64        ┆   ┆ i32        ┆ i32          ┆ i32        ┆ i64     │\n",
      "╞════════════╪═════╪════════╪════════════╪═══╪════════════╪══════════════╪════════════╪═════════╡\n",
      "│ 2019-06-19 ┆ 1   ┆ Avalon ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1210000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-06-13 ┆ 2   ┆ Avalon ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2250000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-06-07 ┆ 3   ┆ Whale  ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2920000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-05-28 ┆ 4   ┆ Avalon ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1530000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "│ 2019-05-22 ┆ 5   ┆ Whale  ┆ 2107       ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 8000000 │\n",
      "│            ┆     ┆ Beach  ┆            ┆   ┆            ┆              ┆            ┆         │\n",
      "└────────────┴─────┴────────┴────────────┴───┴────────────┴──────────────┴────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'TARGET' that's a copy of 'sellPrice'\n",
    "df = df.with_columns(pl.col('sellPrice').alias('TARGET'))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af4a128-3b68-4590-88ef-8e314c6857bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'propType', 'suburbs_encoded', 'Date_right', 'suburb_right', 'postalCode_right', 'sellPrice_right', 'bed_right', 'bath_right', 'car_right', 'suburbs_encoded_right', 'pt_house', 'pt_townhouse', 'pt_duplex/semi-detached', 'pt_villa', 'pt_other', 'pt_terrace', 'pt_warehouse', 'pt_acreage', 'TARGET']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3cd9b-0b4e-4bb1-a18f-0fb58c8ccd1a",
   "metadata": {},
   "source": [
    "## Remove Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222a52ab-636d-46df-b4b0-0791b13627f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 22)\n",
      "┌────────────┬─────┬──────┬─────┬───┬────────────┬──────────────┬────────────┬─────────┐\n",
      "│ postalCode ┆ bed ┆ bath ┆ car ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET  │\n",
      "│ ---        ┆ --- ┆ ---  ┆ --- ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---     │\n",
      "│ i64        ┆ f64 ┆ i64  ┆ f64 ┆   ┆ i32        ┆ i32          ┆ i32        ┆ i64     │\n",
      "╞════════════╪═════╪══════╪═════╪═══╪════════════╪══════════════╪════════════╪═════════╡\n",
      "│ 2107       ┆ 4.0 ┆ 2    ┆ 2.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1210000 │\n",
      "│ 2107       ┆ 4.0 ┆ 3    ┆ 4.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2250000 │\n",
      "│ 2107       ┆ 3.0 ┆ 3    ┆ 2.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 2920000 │\n",
      "│ 2107       ┆ 3.0 ┆ 1    ┆ 2.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 1530000 │\n",
      "│ 2107       ┆ 5.0 ┆ 4    ┆ 4.0 ┆ … ┆ 0          ┆ 0            ┆ 0          ┆ 8000000 │\n",
      "└────────────┴─────┴──────┴─────┴───┴────────────┴──────────────┴────────────┴─────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/6x3yk1r13xvcwchd72ftlnsc0000gn/T/ipykernel_2415/609900798.py:6: DeprecationWarning: named `columns` param is deprecated; use positional `*args` instead.\n",
      "  df_drop = df_drop.drop(columns=columns_to_remove)\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the DataFrame (Polars handles this internally)\n",
    "df_drop = df.clone()\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_remove = [\"Date\", \"Id\", \"suburb\", \"propType\", \"sellPrice\"]\n",
    "df_drop = df_drop.drop(columns=columns_to_remove)\n",
    "\n",
    "# Display the first 5 rows after dropping the columns\n",
    "print(df_drop.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2514117-8297-4b8e-8d3e-3b76120bdb63",
   "metadata": {},
   "source": [
    "## Check null or inf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01607878-c1ca-4281-8262-00fa5cc52f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/6x3yk1r13xvcwchd72ftlnsc0000gn/T/ipykernel_2415/469196574.py:4: DeprecationWarning: The `axis` parameter for `DataFrame.sum` is deprecated. Use `DataFrame.sum_horizontal()` to perform horizontal aggregation.\n",
      "  is_null = df.with_columns(pl.all().is_null().any()).sum(axis=1).sum() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for Null values across the DataFrame\n",
    "is_null = df.with_columns(pl.all().is_null().any()).sum(axis=1).sum() > 0\n",
    "print(\"Is Null: \", is_null)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ff03a7-82df-44b2-946d-c86a81afa129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null: True\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assuming 'df' is your existing Polars DataFrame\n",
    "# Check for Null values\n",
    "contains_null = df.select(pl.any_horizontal(pl.all().is_null().any())).item()\n",
    "\n",
    "# Print the result for Null values\n",
    "print(\"Is Null:\", contains_null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a3c4255-1228-43ea-9d70-4de4bda0dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Inf: False\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Check for Infinite values, only applying it to numeric columns\n",
    "#this will error if you try to run on string columns\n",
    "contains_inf = df.select(\n",
    "    pl.any_horizontal(\n",
    "        pl.col(pl.Float64).is_infinite().any()\n",
    "    )\n",
    ").item()\n",
    "\n",
    "# Print the result for Infinite values\n",
    "print(\"Is Inf:\", contains_inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67b90623-504a-4f31-8077-cde8a663befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 27)\n",
      "┌────────────┬─────┬─────────────┬────────────┬───┬────────────┬─────────────┬────────────┬────────┐\n",
      "│ Date       ┆ Id  ┆ suburb      ┆ postalCode ┆ … ┆ pt_terrace ┆ pt_warehous ┆ pt_acreage ┆ TARGET │\n",
      "│ ---        ┆ --- ┆ ---         ┆ ---        ┆   ┆ ---        ┆ e           ┆ ---        ┆ ---    │\n",
      "│ str        ┆ f64 ┆ str         ┆ f64        ┆   ┆ f64        ┆ ---         ┆ f64        ┆ f64    │\n",
      "│            ┆     ┆             ┆            ┆   ┆            ┆ f64         ┆            ┆        │\n",
      "╞════════════╪═════╪═════════════╪════════════╪═══╪════════════╪═════════════╪════════════╪════════╡\n",
      "│ 2019-06-19 ┆ 1.0 ┆ Avalon      ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 1.21e6 │\n",
      "│            ┆     ┆ Beach       ┆            ┆   ┆            ┆             ┆            ┆        │\n",
      "│ 2019-06-13 ┆ 2.0 ┆ Avalon      ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 2.25e6 │\n",
      "│            ┆     ┆ Beach       ┆            ┆   ┆            ┆             ┆            ┆        │\n",
      "│ 2019-06-07 ┆ 3.0 ┆ Whale Beach ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 2.92e6 │\n",
      "│ 2019-05-28 ┆ 4.0 ┆ Avalon      ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 1.53e6 │\n",
      "│            ┆     ┆ Beach       ┆            ┆   ┆            ┆             ┆            ┆        │\n",
      "│ 2019-05-22 ┆ 5.0 ┆ Whale Beach ┆ 2107.0     ┆ … ┆ 0.0        ┆ 0.0         ┆ 0.0        ┆ 8e6    │\n",
      "└────────────┴─────┴─────────────┴────────────┴───┴────────────┴─────────────┴────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Calculate means for numeric columns\n",
    "numeric_cols = df.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
    "column_means = df.select([pl.col(col).mean() for col in numeric_cols])\n",
    "\n",
    "# Fill NA values with means for numeric columns\n",
    "df_filled = df.with_columns([\n",
    "    pl.col(col).fill_null(column_means.get_column(col)[0])\n",
    "    for col in numeric_cols\n",
    "])\n",
    "\n",
    "print(df_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d4eac3b-e8cc-4839-9d6e-72e267c563af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null: False\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assuming 'df' is your existing Polars DataFrame\n",
    "# Check for Null values\n",
    "contains_null = df_filled.select(pl.any_horizontal(pl.all().is_null().any())).item()\n",
    "\n",
    "# Print the result for Null values\n",
    "print(\"Is Null:\", contains_null)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45026b-015a-438f-aeef-8b9f0c5af5a7",
   "metadata": {},
   "source": [
    "## Remove Redundat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f65fc5-4c53-4db7-968b-374bcef53d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Id', 'suburb', 'postalCode', 'sellPrice', 'bed', 'bath', 'car', 'propType', 'suburbs_encoded', 'Date_right', 'suburb_right', 'postalCode_right', 'sellPrice_right', 'bed_right', 'bath_right', 'car_right', 'suburbs_encoded_right', 'pt_house', 'pt_townhouse', 'pt_duplex/semi-detached', 'pt_villa', 'pt_other', 'pt_terrace', 'pt_warehouse', 'pt_acreage', 'TARGET']\n"
     ]
    }
   ],
   "source": [
    "print(df_filled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a59534bf-e41b-4b51-800d-08b09044714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 19)\n",
      "┌────────────┬─────┬──────┬─────┬───┬────────────┬──────────────┬────────────┬────────┐\n",
      "│ postalCode ┆ bed ┆ bath ┆ car ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET │\n",
      "│ ---        ┆ --- ┆ ---  ┆ --- ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---    │\n",
      "│ f64        ┆ f64 ┆ f64  ┆ f64 ┆   ┆ f64        ┆ f64          ┆ f64        ┆ f64    │\n",
      "╞════════════╪═════╪══════╪═════╪═══╪════════════╪══════════════╪════════════╪════════╡\n",
      "│ 2107.0     ┆ 4.0 ┆ 2.0  ┆ 2.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 1.21e6 │\n",
      "│ 2107.0     ┆ 4.0 ┆ 3.0  ┆ 4.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 2.25e6 │\n",
      "│ 2107.0     ┆ 3.0 ┆ 3.0  ┆ 2.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 2.92e6 │\n",
      "│ 2107.0     ┆ 3.0 ┆ 1.0  ┆ 2.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 1.53e6 │\n",
      "│ 2107.0     ┆ 5.0 ┆ 4.0  ┆ 4.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 8e6    │\n",
      "└────────────┴─────┴──────┴─────┴───┴────────────┴──────────────┴────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Remove features\n",
    "df_drop = df_filled.drop([\"Date\",\"Date_right\", \"Id\", \"suburb\", \"suburb_right\", \"propType\", \"sellPrice_right\", \"sellPrice\"])\n",
    "\n",
    "print(df_drop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3dfd1f0-fc9a-4d43-a298-363abf03efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['postalCode', 'bed', 'bath', 'car', 'suburbs_encoded', 'postalCode_right', 'bed_right', 'bath_right', 'car_right', 'suburbs_encoded_right', 'pt_house', 'pt_townhouse', 'pt_duplex/semi-detached', 'pt_villa', 'pt_other', 'pt_terrace', 'pt_warehouse', 'pt_acreage', 'TARGET']\n"
     ]
    }
   ],
   "source": [
    "print(df_drop.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d39285-4b4d-4b0c-8821-bbef5837d1f5",
   "metadata": {},
   "source": [
    "## Feature Scaling - Min Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c7c58ca-b7a5-43a8-844c-26a200d0fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 19)\n",
      "┌────────────┬──────────┬──────────┬───────┬───┬────────────┬──────────────┬────────────┬────────┐\n",
      "│ postalCode ┆ bed      ┆ bath     ┆ car   ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET │\n",
      "│ ---        ┆ ---      ┆ ---      ┆ ---   ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---    │\n",
      "│ f64        ┆ f64      ┆ f64      ┆ f64   ┆   ┆ f64        ┆ f64          ┆ f64        ┆ f64    │\n",
      "╞════════════╪══════════╪══════════╪═══════╪═══╪════════════╪══════════════╪════════════╪════════╡\n",
      "│ 0.037179   ┆ 0.030612 ┆ 0.010204 ┆ 0.025 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 1.21e6 │\n",
      "│ 0.037179   ┆ 0.030612 ┆ 0.020408 ┆ 0.075 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 2.25e6 │\n",
      "│ 0.037179   ┆ 0.020408 ┆ 0.020408 ┆ 0.025 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 2.92e6 │\n",
      "│ 0.037179   ┆ 0.020408 ┆ 0.0      ┆ 0.025 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 1.53e6 │\n",
      "│ 0.037179   ┆ 0.040816 ┆ 0.030612 ┆ 0.075 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 8e6    │\n",
      "└────────────┴──────────┴──────────┴───────┴───┴────────────┴──────────────┴────────────┴────────┘\n",
      "[Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assuming df_drop is your Polars DataFrame after dropping unnecessary columns\n",
    "df_scaling = df_drop.clone()  # Create a copy of df_drop\n",
    "\n",
    "# Function to check if a column is numeric\n",
    "def is_numeric(col):\n",
    "    return col.dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64]\n",
    "\n",
    "# Convert date columns to timestamps, other string columns to categorical\n",
    "for col in df_scaling.columns:\n",
    "    if df_scaling[col].dtype == pl.Utf8:\n",
    "        if \"Date\" in col:  # Assuming date columns have \"Date\" in their name\n",
    "            df_scaling = df_scaling.with_columns(pl.col(col).str.to_date().cast(pl.Int64).alias(col))\n",
    "        else:\n",
    "            df_scaling = df_scaling.with_columns(pl.col(col).cast(pl.Categorical).cast(pl.Int32).alias(col))\n",
    "\n",
    "# Apply min-max scaling to all numeric columns except TARGET\n",
    "columns_to_scale = [col for col in df_scaling.columns if col != \"TARGET\" and is_numeric(df_scaling[col])]\n",
    "\n",
    "df_scaling = df_scaling.with_columns([\n",
    "    ((pl.col(col) - pl.col(col).min()) / (pl.col(col).max() - pl.col(col).min())).alias(col)\n",
    "    for col in columns_to_scale\n",
    "])\n",
    "\n",
    "print(df_scaling.head())\n",
    "print(df_scaling.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddbfa3-ca1b-465f-84a8-20b20c3f9bf1",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a60d3d-3005-40ac-93bc-c8865aec469b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>postalCode</th><th>bed</th><th>bath</th><th>car</th><th>suburbs_encoded</th><th>postalCode_right</th><th>bed_right</th><th>bath_right</th><th>car_right</th><th>suburbs_encoded_right</th><th>pt_house</th><th>pt_townhouse</th><th>pt_duplex/semi-detached</th><th>pt_villa</th><th>pt_other</th><th>pt_terrace</th><th>pt_warehouse</th><th>pt_acreage</th><th>TARGET</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2107.0</td><td>4.0</td><td>2.0</td><td>2.0</td><td>22.0</td><td>2107.0</td><td>4.0</td><td>2.0</td><td>2.0</td><td>22.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.21e6</td></tr><tr><td>2107.0</td><td>4.0</td><td>3.0</td><td>4.0</td><td>22.0</td><td>2107.0</td><td>4.0</td><td>3.0</td><td>4.0</td><td>22.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.25e6</td></tr><tr><td>2107.0</td><td>3.0</td><td>3.0</td><td>2.0</td><td>654.0</td><td>2107.0</td><td>3.0</td><td>3.0</td><td>2.0</td><td>654.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.92e6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 19)\n",
       "┌────────────┬─────┬──────┬─────┬───┬────────────┬──────────────┬────────────┬────────┐\n",
       "│ postalCode ┆ bed ┆ bath ┆ car ┆ … ┆ pt_terrace ┆ pt_warehouse ┆ pt_acreage ┆ TARGET │\n",
       "│ ---        ┆ --- ┆ ---  ┆ --- ┆   ┆ ---        ┆ ---          ┆ ---        ┆ ---    │\n",
       "│ f64        ┆ f64 ┆ f64  ┆ f64 ┆   ┆ f64        ┆ f64          ┆ f64        ┆ f64    │\n",
       "╞════════════╪═════╪══════╪═════╪═══╪════════════╪══════════════╪════════════╪════════╡\n",
       "│ 2107.0     ┆ 4.0 ┆ 2.0  ┆ 2.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 1.21e6 │\n",
       "│ 2107.0     ┆ 4.0 ┆ 3.0  ┆ 4.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 2.25e6 │\n",
       "│ 2107.0     ┆ 3.0 ┆ 3.0  ┆ 2.0 ┆ … ┆ 0.0        ┆ 0.0          ┆ 0.0        ┆ 2.92e6 │\n",
       "└────────────┴─────┴──────┴─────┴───┴────────────┴──────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use correct Dataframe\n",
    "is_deep_learning = False\n",
    "df_tts = df_scaling.clone() if is_deep_learning else df_drop.clone()\n",
    "df_tts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8a83985-e5a7-414a-8ba5-d1b1110a8072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Values: \n",
      " [[2.107e+03 4.000e+00 2.000e+00 2.000e+00 2.200e+01 2.107e+03 4.000e+00\n",
      "  2.000e+00 2.000e+00 2.200e+01 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.107e+03 4.000e+00 3.000e+00 4.000e+00 2.200e+01 2.107e+03 4.000e+00\n",
      "  3.000e+00 4.000e+00 2.200e+01 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n",
      "y Values: \n",
      " [1210000. 2250000. 2920000. 1530000. 8000000.]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assuming df_tts is already defined as a Polars DataFrame\n",
    "\n",
    "# Split X and y data\n",
    "# X will contain all columns except the last one\n",
    "X = df_tts[:, :-1].to_numpy()\n",
    "\n",
    "# y will contain only the last column\n",
    "y = df_tts[:, -1].to_numpy()\n",
    "\n",
    "# Print the first 2 rows of X\n",
    "print(\"X Values: \\n\", X[:2])\n",
    "\n",
    "# Print the first 5 rows of y\n",
    "print(\"y Values: \\n\", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28fed583-5c99-4fad-ab23-3e58208e5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (179553, 18)\n",
      "X_test shape:  (19951, 18)\n",
      "y_train shape:  (179553,)\n",
      "y_test shape:  (19951,)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "# x_train and y_train are the training datasets\n",
    "# X_test and y_test are the testing datasets\n",
    "# test_size=0.1 means 10% of the data will be used for testing\n",
    "# random_state=1 ensures reproducibility of the split\n",
    "# shuffle=True ensures the data is shuffled before splitting; you can decide if you want recent data or not\n",
    "x_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, shuffle=True)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7fdf3-80dd-4407-ba37-b6d5416aa999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
